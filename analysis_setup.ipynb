{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import main\n",
    "import tweepy\n",
    "import tweet_processing\n",
    "\n",
    "consumer_key = 'JABuSWGGYTBTQ18FjMvrOJgNQ'\n",
    "consumer_secret = '3OBRHyALFYS7eClUBEWrjdtupmhee1u1Tt8GYLjngZ6nTkOGIc'\n",
    "\n",
    "# User access token - Uses user @_tweetsentiment\n",
    "access_token = '921765888591884288-dDXYnS3luFVZe2qcAzjjssTiNNjf7FM'\n",
    "access_token_secret = 'tOAUKdwMdUojotllVeSsGFxOjCZAxdEZjhmheKTdVX1Ne'\n",
    "\n",
    "# OAuth process, using the keys and tokens\n",
    "try:\n",
    "    # create OAuthHandler object\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    # set access token and secret\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # create tweepy API object to fetch tweets\n",
    "    api = tweepy.API(auth)\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")\n",
    "\n",
    "sa = main.SentimentAnalysis(False, api, 'bayes', 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing tweets from Twitter\n",
    "The first step in the setup process will be to gather the data to be analysed. As we are using Twitter as our source in this case, we will be pulling relevent tweets from Twitter. This is done by using the Tweepy library in Python and results in the below tweets from [@_tweetsentiment](https://twitter.com/_tweetsentiment) ready to proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull the tweets from the users timeline on Twitter using the API\n",
    "def pull_user_tweets(user, debug):\n",
    "    user_tweets = sa.api.user_timeline(user, count=sa.tweets_num, tweet_mode='extended')\n",
    "    return user_tweets\n",
    "    \n",
    "def pull_tweets(option, user, search_term):\n",
    "    tweets_to_process = []\n",
    "    # Pull the users actual tweets or their timeline tweets\n",
    "    if option == 'self':\n",
    "        old_tweets = pull_user_tweets(user, False)\n",
    "        for tweet in old_tweets:\n",
    "            if 'retweeted_status' in dir(tweet):\n",
    "                tweets_to_process.append('RT @' + tweet.retweeted_status.author.screen_name + ': \"' + tweet.retweeted_status.full_text + '\"')\n",
    "            else:\n",
    "                tweets_to_process.append('\"' + tweet.full_text + '\"')\n",
    "    # ...Shortened for demonstration purposes\n",
    "    return tweets_to_process\n",
    "\n",
    "unprocessed_tweets = pull_tweets('self', '_tweetsentiment', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "- \"1 of my tweets were identified as positive and 1 were identified as negative. Analyse your tweets now: https://t.co/TFru1psHj5 via @_tweetsentiment\"\n",
      "- \"0 of my tweets were identified as positive and 1 were identified as negative. Analyse your tweets now: https://t.co/TFru1psHj5 via @_tweetsentiment\"\n",
      "- \"The website is currently work in progress and nearing completion, watch this space!\"\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the pulled tweets nicely\n",
    "print(\"-----------------------------------\")\n",
    "for tweet in unprocessed_tweets:\n",
    "    print(\"- \" + tweet)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet preprocessing\n",
    "Now that we have these tweets we will need to preprocess them to remove any 'noise' from them, the noise being any unuseful details from them which don't help us to determine the sentiment.\n",
    "\n",
    "Preprocessing usually involves a number of steps, for tweet preprocessing these steps include:\n",
    "* Stripping links\n",
    "* Stripping special characters\n",
    "* Stripping emojis and emoticons\n",
    "* Stripping @mentions\n",
    "* Trimming repeating words e.g happyyyyyyyyyyy -> happy\n",
    "* Removing common stop words such as the, a, and etc. \n",
    "\n",
    "Below you can see the result when the tweets pulled in the previous step are passed through the preprocessing procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If there are tweets to process then process them\n",
    "if unprocessed_tweets != -1:\n",
    "    # Need to preprocess these tweets now to remove symbols and hashtags etc.\n",
    "    preprocessed_tweets = tweet_processing.preprocess_tweets(unprocessed_tweets, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "- tweets identified positive identified negative analyse tweets via\n",
      "- tweets identified positive identified negative analyse tweets via\n",
      "- the website currently work progress nearing completion watch space\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the processed tweets nicely\n",
    "print(\"-----------------------------------\")\n",
    "for tweet in preprocessed_tweets:\n",
    "    print(\"- \" + tweet)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of preprocessing is a key part of sentiment analysis especially when it comes to Twitter or any social media platform. There will often be a lot of noise which isn't useful to the sentiment analysis process, so by stripping this out beforehand we can improve the overall efficiency of the procedure. \n",
    "\n",
    "Below you can see an example of 3 potential tweets which may be found on Twitter. Now observe the output tweets after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [\"I am happy but i like a lot of cake\", \"i am lol, well whatever\", \"i am sad and i hate all of this\"]\n",
    "preprocessed = tweet_processing.preprocess_tweets(sentences, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "- I am happy but i like a lot of cake\n",
      "- i am lol, well whatever\n",
      "- i am sad and i hate all of this\n",
      "-----------------------------------\n",
      "- happy like lot cake\n",
      "- lol well whatever\n",
      "- sad hate\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print the processed tweets nicely\n",
    "print(\"-----------------------------------\")\n",
    "for tweet in sentences:\n",
    "    print(\"- \" + tweet)\n",
    "print(\"-----------------------------------\")\n",
    "for tweet in preprocessed:\n",
    "    print(\"- \" + tweet)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus selection\n",
    "After pulling the data and preprocessing it, we must choose which training set (corpus) we wish to use in order to train the classifier which we will use in the sentiment analysis. The corpus used for training a sentiment analysis classifier is a key component of the whole process and therefore it is vital that the most optimal corpus is used.\n",
    "\n",
    "For this specific case there is a choice of three corpus':\n",
    "\n",
    "1. Tweets - A set of 100,000 tweets, with their sentiment determined manually\n",
    "2. Tweets - A set of 10,000 tweets taken from the NLTK Twitter Corpus library (5000 positive tweets and 5000 negative tweets) which have had their sentiment determined manually.\n",
    "3. Film reviews - A set of 2000 film reviews with their sentiment determined manually\n",
    "\n",
    "Now there is an obvious difference between the first two corpus' and the third which will without doubt make the first two better. In this case, all three are included for comparison purposes and to demonstrate the importance of the training corpus in the overall process. Below is an small sample taken from the first corpus of tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"is so sad for my APL friend.............\" - Negative\n",
    "- \"I missed the New Moon trailer...\" - Negative\n",
    "- \"omg its already 7:30 :O\" - Positive\n",
    "- \".. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\" - Negative\n",
    "- \"i think mi bf is cheating on me!!!       T_T\" - Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "Now that we have the preprocessed tweets, it is time to classify them. On this site there are currently 2 options to do this:\n",
    "* Naive Bayesian Classifier\n",
    "* Support Vector Machine (SVM)\n",
    "\n",
    "In the next couple of sections I will explain how we can now go on to classify these tweets using a Naive Bayesian or Support vector machine classifier."
   ]
  }
 ],
 "metadata": {
  "cell.metadata.hide_input": "true",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
