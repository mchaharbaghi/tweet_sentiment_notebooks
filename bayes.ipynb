{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import math\n",
    "import tweepy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import main\n",
    "import tweet_processing\n",
    "import classifier\n",
    "\n",
    "api = \"Dummy\" # We don't need the actual API here so just pass a dummy value in\n",
    "\n",
    "sa = main.SentimentAnalysis(False, api, 'bayes', 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and using a Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the sentiment analysis process is to train a Naive Bayesian Classifier using the chosen training corpus. First we'll go in to a bit of detail about what a Naive Bayesian Classifier is exactly, if you want more information you can check out [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Naive Bayesian Classifier?\n",
    "Naive Bayesian Classifiers are a group of classification algorithms which are based upon [Bayes Theorem](http://en.wikipedia.org/wiki/Bayes%27_theorem). The main idea behind this classifier is that the presence of every feature to be classified is completely independent of the presence of any other feature.\n",
    "\n",
    "The best way to try and explain how this classifier works is by way of an example.\n",
    "If we have a piece of fruit, we can consider that this piece of fruit is an orange if it has the following 3 properties:\n",
    "* Orange in colour\n",
    "* Round in shape\n",
    "* Around 8cm in diameter\n",
    "\n",
    "A Naive Bayesian Classifier aiming to determine if a piece of fruit is an orange will consider that each of these 3 properties or \"features\" will contribute independently to the probability that the piece of fruit is an orange. \n",
    "Like most things, Naive Bayes has its pros and cons. It is both fast and easily trained however the assumption it makes about every feature being independent isn't always the case and thus is where the naive part came from in its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n",
    "Now we have a better understanding of how the Naive Bayesian Classifier works, it is time to train it using a training corpus of preclassified data.\n",
    "\n",
    "The first step is to transform each entry in the training corpus in to a set of features by seperating each entry, word by word. Below you can see an example of this step using a small, sample corpus and the output it gives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'happy': True}, 'positive']\n",
      "[{'excited': True, 'i': True, 'am': True, 'very': True}, 'positive']\n",
      "[{'lol': True, 'yay': True, 'exciting': True}, 'positive']\n",
      "[{'happy': True, 'time': True, 'have': True, 'cake': True, 'i': True}, 'positive']\n",
      "[{'amazing': True, 'this': True, 'is': True}, 'positive']\n",
      "[{'i': True, 'sad': True, 'am': True, 'no': True, 'so': True}, 'negative']\n",
      "[{'my': True, 'died': True, 'cat': True, 'just': True}, 'negative']\n",
      "[{'Just': True, 'my': True, 'car': True, 'crashed': True, 'whoops': True}, 'negative']\n",
      "[{'Late': True, 'job': True, 'my': True, 'new': True, 'for': True, 'arghh': True}, 'negative']\n",
      "[{'crying': True, 'like': True, 'I': True, 'feel': True}, 'negative']\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = [[\"happy :)\", \"positive\"],[\"i am very excited!\", \"positive\"],[\"yay lol, exciting!\", \"positive\"], [\"happy time, i have cake!\", \"positive\"], [\"this is amazing! :)\", \"positive\"]]\n",
    "negative_tweets = [[\"no i am so sad\", \"negative\"], [\"my cat just died :(\", \"negative\"], [\"Just crashed my car whoops!\", \"negative\"], [\"Late for my new job arghh\", \"negative\"], [\"I feel like crying!\", \"negative\"]]        \n",
    "\n",
    "tweets = positive_tweets + negative_tweets\n",
    "\n",
    "def format_sentence(sent): \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return {word: True for word in set(tokenizer.tokenize(sent))}\n",
    "\n",
    "count = 0\n",
    "for entry in tweets:\n",
    "    text = entry[0]\n",
    "    sentiment = entry[1]\n",
    "    entry = [format_sentence(text), sentiment]\n",
    "    tweets[count] = entry\n",
    "    count += 1\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated the features for each entry all that is left then is to use them to train the classifier. In code terms this is as straightforward as calling the train method with our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the overall process in terms of a diagram:\n",
    "\n",
    "\n",
    "![Bayes diagram](https://www.laurentluce.com/images/blog/nltk/overview.png)\n",
    "<center>[(Image Source)](https://www.tweetsentiment.co.uk/static/images/bayes.png)</center>\n",
    "\n",
    "\n",
    "In our example above, the word features and feature extraction steps are combined into a single step which in this case is the method 'format_sentence' from previously.\n",
    "\n",
    "The classifier uses the prior probability of each label, this is the number of times each label occurs in the training set, and the contribution that each feature provides. \n",
    "In our case, the frequency of each label is the same for 'positive' and 'negative'. The word 'amazing' appears in 1 of 5 of the positive tweets and none of the negative tweets. This means that the likelihood of the ‘positive’ label will be multiplied by 0.2 when this word is seen in the analysis input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying tweets\n",
    "Now that we have the classifier trained we can use it to classify tweets pulled from Twitter. Lets try this out for perhaps a trivial example tweet \"My hamster just died\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My hamster just died\"\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "tweet = \"My hamster just died\"\n",
    "print('\"' + tweet + '\"')\n",
    "predict = classifier.classify(tweet_processing.format_sentence(tweet))\n",
    "print(\"Sentiment:\", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also return the positive and negative sentiment percentages from the classifier for each classification call. \n",
    "\n",
    "If the difference is small between the 2 values (Less than 10%) then we say the tweet is of neutral sentiment instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 10%\n",
      "Negative: 90%\n"
     ]
    }
   ],
   "source": [
    "dist = classifier.prob_classify(tweet_processing.format_sentence(tweet))\n",
    "\n",
    "for label in dist.samples():\n",
    "    if label == 'positive':\n",
    "        positive = dist.prob(label)\n",
    "    elif label == 'negative':\n",
    "        negative = dist.prob(label)\n",
    "        \n",
    "if positive - negative < 0.1 and positive - negative > -0.1:\n",
    "    predict = \"neutral\"\n",
    "\n",
    "positive = '%.1d' % (round(positive, 2) * 100) + \"%\"\n",
    "negative = '%.1d' % (round(negative, 2) * 100) + \"%\"\n",
    "print(\"Positive:\", positive)\n",
    "print(\"Negative:\", negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to classification of a tweet, the first step is to break the tweet down in to its features in the same way that we did for our training corpus earlier. Doing this gives us the features of this tweet which we can then pass to the classifier for classification.\n",
    "\n",
    "The next step is to find the logarithmic probability for each label. For our case the probability of each label (positive and negative) is 0.5. The logarithmic probability is $Log_{2}$ of that which is -1 so our probability set after this step looks like:\n",
    "\n",
    "```python\n",
    "\n",
    "{'positive': -1.0, 'negative': -1.0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.5\n",
      "Negative: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive:\", classifier._label_probdist.prob('positive'))\n",
    "print(\"Negative:\", classifier._label_probdist.prob('negative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the logarithmic probability of the features given labels to this set. For each label in our features, we go through the feature set and we add the logarithmic probability of each item to our probability set from above. For example, we have the feature name 'died' and the feature value 'True'. Its probability value for the label 'positive' in our classifier is -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('positive', 'lol'): <ELEProbDist based on 5 samples>, ('negative', 'whoops'): <ELEProbDist based on 5 samples>, ('positive', 'died'): <ELEProbDist based on 5 samples>, ('negative', 'just'): <ELEProbDist based on 5 samples>,...\n",
      "\n",
      "Feature probability distribution value for label 'died': 0.25\n",
      "...converted to a log value:  -2.0\n"
     ]
    }
   ],
   "source": [
    "print(str(classifier._feature_probdist)[0:225] + \"...\\n\") # Print a small section from the start for demo purposes\n",
    "\n",
    "print(\"Feature probability distribution value for label 'died':\", classifier._feature_probdist[('negative', 'died')].prob(True))\n",
    "\n",
    "print(\"...converted to a log value: \", math.log(classifier._feature_probdist[('negative', 'died')].prob(True), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done this for every label, we will then have a dictionary of probability distribution which will give us the label with the greatest probability. In this case it is negative indicating that our classifier has told us it thinks the tweet has negative sentiment.\n",
    "\n",
    "\n",
    "If we then try and use this classifier to classify the tweet \"You're annoying\", you'll see that it returns 'neutral' even though it quite clearly isn't. This is down to the fact that there is no information about the feature 'annoying' in our training set. For this reason a well chosen and larger training set is highly desirable in order to ensure higher accuracy when it comes to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You're annoying\"\n",
      "Sentiment: neutral\n",
      "Positive: 50%\n",
      "Negative: 50%\n"
     ]
    }
   ],
   "source": [
    "tweet = \"You're annoying\"\n",
    "print('\"' + tweet + '\"')\n",
    "dist = classifier.prob_classify(tweet_processing.format_sentence(tweet))\n",
    "for label in dist.samples():\n",
    "    if label == 'positive':\n",
    "        positive = dist.prob(label)\n",
    "    elif label == 'negative':\n",
    "        negative = dist.prob(label)\n",
    "        \n",
    "predict = classifier.classify(tweet_processing.format_sentence(tweet))       \n",
    "        \n",
    "if positive - negative < 0.1 and positive - negative > -0.1:\n",
    "    predict = \"neutral\"\n",
    "\n",
    "positive = '%.1d' % (round(positive, 2) * 100) + \"%\"\n",
    "negative = '%.1d' % (round(negative, 2) * 100) + \"%\"\n",
    "print(\"Sentiment: \" + predict)\n",
    "print(\"Positive:\", positive)\n",
    "print(\"Negative:\", negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
