{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import main\n",
    "import tweepy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "api = \"Dummy\" # We don't need the actual API here so just pass a dummy value in\n",
    "\n",
    "sa = main.SentimentAnalysis(False, api, 'svm', 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and using an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is an SVM classifier?\n",
    "The second option for classifying our preprocessed tweets is a Support vector machine or SVM for short. \n",
    "\n",
    "A support vector machine is a supervised learning model so it looks at labeled training data and uses this to categorise any new data based on the categories which appear in the training data. Don't worry if that doesn't quite make sense to you, I'll break it down a bit more below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM classifier\n",
    "If our training corpus is too large to allow us to run sentiment analysis in a reasonable time then we can trim it to a more manageable size. In this case I'll use a fairly trivial set of 10 tweets for our training corpus, 5 positive and 5 negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- happy :)\n",
      "- i am very excited!\n",
      "- yay lol, exciting!\n",
      "- happy time, i have cake!\n",
      "- this is amazing! :)\n",
      "- no i am so sad\n",
      "- my cat just died :(\n",
      "- Just crashed my car whoops!\n",
      "- Late for my new job arghh\n",
      "- I feel like crying!\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = [[\"happy :)\", \"positive\"],[\"i am very excited!\", \"positive\"],[\"yay lol, exciting!\", \"positive\"], [\"happy time, i have cake!\", \"positive\"], [\"this is amazing! :)\", \"positive\"]]\n",
    "negative_tweets = [[\"no i am so sad\", \"negative\"], [\"my cat just died :(\", \"negative\"], [\"Just crashed my car whoops!\", \"negative\"], [\"Late for my new job arghh\", \"negative\"], [\"I feel like crying!\", \"negative\"]]        \n",
    "\n",
    "tweets = [i[0] for i in positive_tweets] + [i[0] for i in negative_tweets]\n",
    "labels = [i[1] for i in positive_tweets] + [i[1] for i in negative_tweets]\n",
    "\n",
    "sa.training_set_size = min(len(tweets),  sa.svm_training_set_size)\n",
    "\n",
    "# Trim the training corpus to a smaller more manageable size\n",
    "tweets = tweets[:int(sa.training_set_size)]\n",
    "labels = labels[:int(sa.training_set_size)]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(\"-\", tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create our vectorizer object which will convert our trimmed training corpus in to a set of vectors using the *tf-idf* statistic. \n",
    "\n",
    "You can [see here](http://www.tfidf.com/) for more details on tf-idf but to summarise, it stands for **term frequency - inverse document frequency** and it is a statistic that aims to reflect how important a word is to a document in a collection, or in this case, a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1,\n",
    "                            max_df=0.95,\n",
    "                            sublinear_tf = True,\n",
    "                            use_idf = True,\n",
    "                            ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you toggle on the code, above you can see the vectorizer setup and that we have a number of parameters for this.\n",
    "* min_df - This basically states that we want to discard any words which appear less than some value of times. In the live code this value is set to 5 but for demonstration purposes as we have a very small training set the parameter has been set to 1.\n",
    "* max_df - This allows us to discard any word which appear in more than X% of our documents.\n",
    "* sublinear_tf - This applies sublinear term frequency scaling, i.e. replaces term frequency with 1 +    $Log_{2}$(term frequency).\n",
    "* use_idf - The final parameter allows us to choose whether we wish to use inverse document frequency or not, so scaling frequency values down significantly as we use $Log_{2}$ values instead ([See here](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html) for more details on inverse document frequency).\n",
    "* ngram_range - The lower and upper boundary of values for n for different n-grams which can be extracted from each text entry. N-grams are all combinations of adjacent words or letters of length n that you can find in the source text. You can [see here](https://en.wikipedia.org/wiki/N-gram) for more on N-grams.\n",
    "\n",
    "For more details on the TfidfVectorizer parameters you can [see here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vectorizer we can go ahead and use it to create our training vectors by transforming our trimmed training corpus into a set of training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the setup phase is to create our classifier. \n",
    "\n",
    "The Python library *scikit-learn* comes with a number of different classifiers already built-in. In these experiments, we will use the LinearSVC (linear support vector classification) variation of Support Vector Machine (SVM). \n",
    "\n",
    "We pass an argument C = 0.1 to the SVM object. Don't worry about this for now, we'll come back to it later on. \n",
    "\n",
    "Once this has been done we can simply fit our training vectors and corresponding sentiment labels from above in it and we are ready to classify new tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform classification with SVM\n",
    "classifier_linear = svm.LinearSVC(C=0.1)\n",
    "\n",
    "classifier_linear.fit(train_vectors, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying using the SVM classifier\n",
    "\n",
    "Now all we are ready to analyse new tweets. For demonstration purposes we will take a trivial tweet from a sentiment point of view and analyse it to obtain the sentiment.\n",
    "\n",
    "We do this by transforming the tweet in to a test vector using our vectorizer again and then pass this vector to our classifier to predict the sentiment by plotting it on to the graph and see which side of the hyperplane the new vector lies. \n",
    "\n",
    "The hyperplane is the line through our graph which satisfies two properties:\n",
    "1. It has the maximum possible distance between the closest point either side of the line to the line\n",
    "2. It correctly seperates as many points by class as possible\n",
    "\n",
    "![SVM diagram](https://www.tweetsentiment.co.uk/static/images/svm.jpg)\n",
    "<center>[(Image source)](http://blogs.quickheal.com/machine-learning-approach-advanced-threat-hunting/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that SVM parameter C from above? The value that this has determines how much we want to focus on property 2 from above. A higher C value means that a smaller-margin hyperplane will be chosen in order to try and correctly identify as many points as possible. The inverse is also true for smaller C values. The technical name of this parameter is the penalty parameter C of the error term.\n",
    "\n",
    "Below you can see 2 SVM examples demonstrating the difference between a lower C value and a higher C value.\n",
    "\n",
    "![Low and high C diagram](https://www.tweetsentiment.co.uk/static/images/svm_c_values.png)\n",
    "<center>[(Image source)](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see for our trivial tweet, our SVM correctly predicts the sentiment to be positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I am very happy :)\"\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I am very happy :)\"\n",
    "test_vector = vectorizer.transform([tweet])\n",
    "prediction = classifier_linear.predict(test_vector)\n",
    "\n",
    "print('\"' + tweet + '\"')\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can also return the positive and negative sentiment percentages from the classifier for each classification call:\n",
    "\n",
    "Again if the difference is less than 10% we will classify the sentiment as neutral instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 59%\n",
      "Negative: 41%\n"
     ]
    }
   ],
   "source": [
    "probs = classifier_linear.decision_function(test_vector)\n",
    "negative = (1 - probs) / 2\n",
    "positive = 1 - negative\n",
    "\n",
    "positive = '%.1d' % (round(positive[0], 2) * 100) + \"%\"\n",
    "negative = '%.1d' % (round(negative[0], 2) * 100) + \"%\"\n",
    "print(\"Positive:\", positive)\n",
    "print(\"Negative:\", negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the final example below you can again see that this tweet is analysed as being neutral when it once again isn't. As was the case for our Bayesian Classifier, the training set we use to train our SVM classifier is just as vital. There is no occurance of the word 'annoying' in our training set so the classification result will be highly likely to be incorrect.\n",
    "\n",
    "This serves to reinforce that a well chosen and larger training set is highly desirable in order to ensure higher accuracy when it comes to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You're annoying\"\n",
      "Sentiment: neutral\n",
      "Positive: 50%\n",
      "Negative: 50%\n"
     ]
    }
   ],
   "source": [
    "tweet = \"You're annoying\"\n",
    "test_vector = vectorizer.transform([tweet])\n",
    "prediction = classifier_linear.predict(test_vector)\n",
    "probs = classifier_linear.decision_function(test_vector)\n",
    "negative = (1 - probs) / 2\n",
    "positive = 1 - negative\n",
    "\n",
    "predict = prediction[0]\n",
    "if positive - negative < 0.1 and positive - negative > -0.1:\n",
    "    predict = \"neutral\"\n",
    "    \n",
    "positive = '%.1d' % (round(positive[0], 2) * 100) + \"%\"\n",
    "negative = '%.1d' % (round(negative[0], 2) * 100) + \"%\"\n",
    "\n",
    "print('\"' + tweet + '\"')\n",
    "print(\"Sentiment:\", predict)\n",
    "print(\"Positive:\", positive)\n",
    "print(\"Negative:\", negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
